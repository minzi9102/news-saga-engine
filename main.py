# main.py
import asyncio
from src.date_utils import get_target_date_str
from src.crawler import CrawlerService
from src.manager import SagaManager
from src.reporter import SagaReporter
from src.archiver import DataArchiver

async def main():
    # 1. ç¡®å®šæ—¥æœŸ
    date_str = get_target_date_str()
    print(f"=== å¯åŠ¨ News Saga Engine: {date_str} ===")
    
    # 2. çˆ¬å–æ•°æ® (Eyes)
    crawler = CrawlerService()
    briefing = await crawler.fetch_daily_briefing(date_str)
    
    if not briefing:
        print("âŒ çˆ¬å–å¤±è´¥æˆ–å½“æ—¥æ— æ–°é—»")
        return

    print(f"âœ… çˆ¬å–å®Œæˆï¼Œå…± {len(briefing.news_items)} æ¡æ–°é—»ã€‚")

    # 2.5 ç«‹å³å½’æ¡£ (Memory - Raw)
    archiver = DataArchiver()
    archiver.save_daily_raw(briefing)

    # 3. è®¤çŸ¥å¤„ç†ä¸å½’æ¡£ (Brain + Memory - Sagas)
    print("ğŸ§  è¿›å…¥è®¤çŸ¥å±‚å¤„ç†...")
    manager = SagaManager()
    await manager.process_daily_briefing(briefing)

    # 4. ç”Ÿæˆå±•ç¤ºå±‚æŠ¥å‘Š (The Face)
    print("\n>>> é˜¶æ®µ 4: ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š")
    reporter = SagaReporter()
    
    # [ä¿®æ”¹] è¿™é‡Œå°† briefing ä¼ å…¥ï¼Œä»¥ä¾¿æ¸²æŸ“â€œä»Šæ—¥åŸå§‹æ¡£æ¡ˆâ€åŒºåŸŸ
    reporter.generate_readme("README.md", briefing=briefing)
    
    print("\n=== å…¨éƒ¨ä»»åŠ¡å®Œæˆ ===")

if __name__ == "__main__":
    asyncio.run(main())